{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extra-1: 딥러닝의 기초 (9)\n",
        "\n",
        "최규빈  \n",
        "2022-12-13\n",
        "\n",
        "> 벡터미분, 역전파와 기울기 소멸\n",
        "\n",
        "# 강의영상\n",
        "\n",
        "> 벡터미분:\n",
        "> <https://youtube.com/playlist?list=PLQqh36zP38-xYcyO28UluDhECTrTDDxCg>\n",
        "\n",
        "> 역전파와 기울기소멸:\n",
        "> <https://youtube.com/playlist?list=PLQqh36zP38-wQ4BAP4n5Nq9xWhHjUcL7J>\n",
        "\n",
        "이 강의는 2021년 빅데이터분석의 강의노트 및 강의영상을 편집하여\n",
        "만들었습니다.\n",
        "\n",
        "# import"
      ],
      "id": "abcda783-9315-478d-96d3-ca1d48768188"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch "
      ],
      "id": "217616da-3e8c-4391-b402-65b3f42d71bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 벡터미분\n",
        "\n",
        "`-` 벡터미분에 대한 강의노트:\n",
        "\n",
        "-   <https://github.com/guebin/DL2022/blob/main/posts/II.%20DNN/supp.pdf>\n",
        "\n",
        "`-` 요약: 회귀분석에서 손실함수에 대한 미분은 아래와 같은 과정으로\n",
        "계산할 수 있다.\n",
        "\n",
        "-   $loss = ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\bf W} - {\\bf W}^\\top {\\bf X}^\\top {\\bf y} + {\\bf W}^\\top {\\bf X}^\\top {\\bf X} {\\bf W}$\n",
        "\n",
        "-   $\\frac{\\partial }{\\partial {\\bf W}}loss = -2{\\bf X}^\\top {\\bf y} +2 {\\bf X}^\\top {\\bf X} {\\bf W}$\n",
        "\n",
        "# 체인룰\n",
        "\n",
        "`-` 체인룰: 어려운 하나의 미분을 손쉬운 여러개의 미분으로 나누는 기법\n",
        "\n",
        "`-` 손실함수가 사실 아래와 같은 변환을 거쳐서 계산되었다고 볼 수 있다.\n",
        "\n",
        "-   ${\\bf X} \\to {\\bf X}{\\bf W} \\to {\\bf y} -{\\bf X}{\\bf W} \\to ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$\n",
        "\n",
        "`-` 위의 과정을 수식으로 정리해보면 아래와 같다.\n",
        "\n",
        "-   ${\\bf u}={\\bf X}{\\bf W}$, $\\quad {\\bf u}: n \\times 1$\n",
        "\n",
        "-   ${\\bf v} = {\\bf y}- {\\bf u},$ $\\quad {\\bf v}: n \\times 1$\n",
        "\n",
        "-   $loss={\\bf v}^\\top {\\bf v},$ $\\quad loss: 1 \\times 1$\n",
        "\n",
        "`-` 손실함수에 대한 미분은 아래와 같다.\n",
        "\n",
        "$$\\frac{\\partial }{\\partial {\\bf W}} loss = \\frac{\\partial }{\\partial {\\bf W}} {\\bf v}^\\top {\\bf v}$$\n",
        "\n",
        "(그런데 이걸 어떻게 계산함?)\n",
        "\n",
        "`-` 계산할 수 있는것들의 모음..\n",
        "\n",
        "-   $\\frac{\\partial}{\\partial {\\bf v}} loss = 2{\\bf v}$ $\\quad \\to$\n",
        "    (n,1) 벡터\n",
        "\n",
        "-   $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top = -{\\bf I}$\n",
        "    $\\quad \\to$ (n,n) 매트릭스\n",
        "\n",
        "-   $\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top = {\\bf X}^\\top$\n",
        "    $\\quad \\to$ (p,n) 매트릭스\n",
        "\n",
        "`-` 혹시.. 아래와 같이 쓸 수 있을까?\n",
        "\n",
        "$$ \\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n",
        "\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n",
        "\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) = \n",
        "\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n",
        "\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n",
        "\\frac{\\partial loss}{\\partial \\bf v}\n",
        "$$\n",
        "\n",
        "-   가능할것 같다. 뭐 기호야 정의하기 나름이니까!\n",
        "\n",
        "`-` 그렇다면 혹시 아래와 같이 쓸 수 있을까?\n",
        "\n",
        "$$\n",
        "\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n",
        "\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n",
        "\\frac{\\partial loss}{\\partial \\bf v} = \\frac{\\partial loss }{\\partial\\bf W}=\\frac{\\partial }{\\partial \\bf W} loss\n",
        "$$\n",
        "\n",
        "-   이건 선을 넘는 것임.\n",
        "-   그런데 어떠한 공식에 의해서 가능함. 그 공식 이름이 체인룰이다.\n",
        "\n",
        "`-` 결국 정리하면 아래의 꼴이 되었다.\n",
        "\n",
        "$$\\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n",
        "\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n",
        "\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) \n",
        "= \n",
        "\\frac{\\partial }{\\partial \\bf W}loss $$\n",
        "\n",
        "`-` 그렇다면?\n",
        "\n",
        "$$\\left({\\bf X}^\\top  \\right) \n",
        "\\left(-{\\bf I} \\right) \n",
        "\\left(2{\\bf v}\\right) \n",
        "= \n",
        "\\frac{\\partial }{\\partial \\bf W}loss $$\n",
        "\n",
        "그런데, ${\\bf v}={\\bf y}-{\\bf u}={\\bf y} -{\\bf X}{\\bf W}$ 이므로\n",
        "\n",
        "$$-2{\\bf X}^\\top\\left({\\bf y}-{\\bf X}{\\bf W}\\right) \n",
        "= \n",
        "\\frac{\\partial }{\\partial \\bf W}loss $$\n",
        "\n",
        "정리하면\n",
        "\n",
        "$$\\frac{\\partial }{\\partial \\bf W}loss = -2{\\bf X}^\\top{\\bf y}+2{\\bf X}^\\top {\\bf X}{\\bf W}$$\n",
        "\n",
        "## 예시: 2021 빅데이터분석 중간고사 문제 2-(b)\n",
        "\n",
        "`-` 미분계수를 계산하는 문제였음..\n",
        "\n",
        "-   <https://guebin.github.io/BDA2021/2021/11/09/mid.html>\n",
        "\n",
        "`-` 체인룰을 이용하여 미분계수를 계산하여 보자."
      ],
      "id": "2434a214-f731-4989-83c6-5be6f8824006"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "ones= torch.ones(5)\n",
        "x = torch.tensor([11.0,12.0,13.0,14.0,15.0])\n",
        "X = torch.vstack([ones,x]).T\n",
        "y = torch.tensor([17.7,18.5,21.2,23.6,24.2])"
      ],
      "id": "69dc749e-b596-4353-9d1b-4f376cd239e2"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "W = torch.tensor([3.0,3.0]) "
      ],
      "id": "73fda8da-e858-47a7-9c19-68ab7e2f0cd8"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "u = X@W \n",
        "v = y-u \n",
        "loss = v.T @ v "
      ],
      "id": "97bd4b74-a730-4ec1-a76b-751fe19cd568"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss"
      ],
      "id": "97ff1f54-5059-414e-ac49-68618f3284cd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` \\$loss \\$ 의 계산"
      ],
      "id": "ec6c0dc9-1e92-46e0-a606-ffba20be2eb6"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.T @ -torch.eye(5) @ (2*v) "
      ],
      "id": "b1700538-b4b0-41f4-909e-e010c4c42fd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 참고로 중간고사 답은"
      ],
      "id": "09a59bdd-eb03-48cf-9a30-4dda76b24c1d"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.T @ -torch.eye(5)@ (2*v) / 5 "
      ],
      "id": "e6016230-fa9d-49fe-ab24-52a4bbc183f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "입니다.\n",
        "\n",
        "`-` 확인"
      ],
      "id": "ff8fe30b-70e9-40ec-8dd4-a80e3f267f11"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "_W = torch.tensor([3.0,3.0],requires_grad=True) "
      ],
      "id": "f6003451-0365-4cb4-8a6d-d05749e2f6f1"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "_loss = (y-X@_W).T @ (y-X@_W)"
      ],
      "id": "bfdcdefd-f53c-43cd-a781-3ebe386eff3b"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "_loss.backward()"
      ],
      "id": "579ab062-5062-4d4c-bc69-3e65be0c3034"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "_W.grad.data"
      ],
      "id": "3124d0ed-9b38-4521-a286-19e474404013"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` $\\frac{\\partial}{\\partial \\bf v} loss= 2{\\bf v}$ 임을 확인하라."
      ],
      "id": "92c2ee8f-a5c0-449e-99fa-41fd7da24876"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "v"
      ],
      "id": "f0c99dfc-d8d9-4667-99d8-85f2dd049260"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "_v= torch.tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000],requires_grad=True)"
      ],
      "id": "36416d0c-642c-4a97-9996-bd43823aed17"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "_loss = _v.T @ _v "
      ],
      "id": "7463c3b3-6156-40ea-8668-7d57923d084f"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "_loss.backward() "
      ],
      "id": "f9af246f-c445-4539-8b81-1cd76f6ef6bb"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "_v.grad.data, v "
      ],
      "id": "a89d62dc-d4c7-4cb6-8ed7-ec72f3076446"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` $\\frac{\\partial }{\\partial {\\bf u}}{\\bf v}^\\top$ 의 계산"
      ],
      "id": "e01070cf-5f8b-4542-bea7-2742fa24f92d"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
        "_u"
      ],
      "id": "41878d75-2399-4e1e-9884-73fd35b4c988"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "_v = y - _u ### 이전의 _v와 또다른 임시 _v "
      ],
      "id": "629d401f-e176-456d-a892-aba0fad18911"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "(_v.T).backward()"
      ],
      "id": "fd503f1e-091d-486a-b9b3-b472277fdf91"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   사실 토치에서는 스칼라아웃풋에 대해서만 미분을 계산할 수 있음\n",
        "\n",
        "그런데\n",
        "$\\frac{\\partial}{\\partial {\\bf u}}{\\bf v}^\\top=\\frac{\\partial}{\\partial {\\bf u}}(v_1,v_2,v_3,v_4,v_5)=\\big(\\frac{\\partial}{\\partial {\\bf u}}v_1,\\frac{\\partial}{\\partial {\\bf u}}v_2,\\frac{\\partial}{\\partial {\\bf u}}v_3,\\frac{\\partial}{\\partial {\\bf u}}v_4,\\frac{\\partial}{\\partial {\\bf u}}v_5\\big)$\n",
        "이므로\n",
        "\n",
        "조금 귀찮은 과정을 거친다면 아래와 같은 알고리즘으로 계산할 수 있다.\n",
        "\n",
        "1.  $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top$의 결과를 저장할\n",
        "    매트릭스를 만든다. 적당히 `A`라고 만들자.\n",
        "\n",
        "2.  `_u` 하나를 임시로 만든다. 그리고 $v_1$을 `_u`로 미분하고 그 결과를\n",
        "    `A`의 첫번째 칼럼에 기록한다.\n",
        "\n",
        "3.  `_u`를 또하나 임시로 만들고 $v_2$를 `_u`로 미분한뒤 그 결과를 `A`의\n",
        "    두번째 칼럼에 기록한다.\n",
        "\n",
        "4.  (1)-(2)와 같은 작업을 $v_5$까지 반복한다.\n",
        "\n",
        "***(0)을 수행***"
      ],
      "id": "8bb19f45-6f39-4842-b133-0b75d6ca139c"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = torch.zeros((5,5))\n",
        "A"
      ],
      "id": "6f3e7f9c-c22d-4a71-b185-64d3bab3318d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***(1)을 수행***"
      ],
      "id": "a458f001-b953-4231-8f10-66f48bfa6e9b"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "u,v "
      ],
      "id": "c4afa843-9af7-4d00-b739-f706934933c7"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
        "v1 = (y-_u)[0]"
      ],
      "id": "bc1e08db-3c27-4fbf-b32d-f87ce6f2bd89"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   이때 $v_1=g(f({\\bf u}))$와 같이 표현할 수 있다. 여기에서\n",
        "    $f((u_1,\\dots,u_5)^\\top)=(y_1-u_1,\\dots,y_5-u_5)^\\top$, 그리고\n",
        "    $g((v_1,\\dots,v_n)^\\top)=v_1$ 라고 생각한다. 즉 $f$는 벡터 뺄셈을\n",
        "    수행하는 함수이고, $g$는 프로젝션 함수이다. 즉\n",
        "    $f:\\mathbb{R}^5 \\to \\mathbb{R}^5$인 함수이고,\n",
        "    $g:\\mathbb{R}^5 \\to \\mathbb{R}$인 함수이다."
      ],
      "id": "e2fa3bec-eaac-448d-b690-c1cf49024bde"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "v1.backward()"
      ],
      "id": "181939d0-a5de-453e-a576-d8f25698dc82"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "_u.grad.data"
      ],
      "id": "72319636-170a-4730-bee6-9988284f1117"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "A[:,0]= _u.grad.data"
      ],
      "id": "7237e87d-acea-4dd5-ba2d-29101714c02c"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "A"
      ],
      "id": "e5cf043b-2eec-40d2-a2f9-c66a89868c53"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***(2)를 수행***"
      ],
      "id": "5a1d35f8-aab8-4826-a6c3-ff19f923bafa"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
        "v2 = (y-_u)[1]"
      ],
      "id": "e4ce63e8-ad56-4bb2-943d-94e8585c3e05"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "v2.backward()"
      ],
      "id": "b69505a2-1fe1-4089-a61c-49da015d75cc"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "_u.grad.data"
      ],
      "id": "68042b18-2a87-4c20-9378-992a3ac0c6b4"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "A[:,1]= _u.grad.data\n",
        "A"
      ],
      "id": "06ec45c0-b578-4618-9965-eedc5a833e07"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***(3)을 수행*** // 그냥 (1)~(2)도 새로 수행하자."
      ],
      "id": "0609fd35-e936-4f80-8c46-5ac2ac6594cd"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5): \n",
        "    _u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
        "    _v = (y-_u)[i]\n",
        "    _v.backward()\n",
        "    A[:,i]= _u.grad.data"
      ],
      "id": "6f2b31cf-b516-4778-913d-ae6b844d4091"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "A"
      ],
      "id": "ddd53a8d-5749-41ce-bd55-14b845372ac7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   이론적인 결과인 $-{\\bf I}$와 일치한다.\n",
        "\n",
        "`-` $\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top$의 계산\n",
        "\n",
        "$\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top = \\frac{\\partial }{\\partial {\\bf W}}(u_1,\\dots,u_5)=\\big(\\frac{\\partial }{\\partial {\\bf W}}u_1,\\dots,\\frac{\\partial }{\\partial {\\bf W}}u_5 \\big)$"
      ],
      "id": "afcc50a5-765f-4c4e-83b7-d97cba3bd30c"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "B = torch.zeros((2,5))\n",
        "B"
      ],
      "id": "00ab6d57-15e5-4795-9e3d-7096a634c35d"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "W"
      ],
      "id": "7d41741d-71d4-49ce-88d3-0c822829aa82"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "_W = torch.tensor([3., 3.],requires_grad=True)\n",
        "_W"
      ],
      "id": "b514297b-a61e-46cb-a69a-c958fecd60ac"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5): \n",
        "    _W = torch.tensor([3., 3.],requires_grad=True)\n",
        "    _u = (X@_W)[i]\n",
        "    _u.backward()\n",
        "    B[:,i]= _W.grad.data"
      ],
      "id": "d8ea85c9-2524-4ea3-9c26-4f3474f5482d"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "B # X의 트랜스포즈"
      ],
      "id": "3eb14f57-0152-4079-9581-c712da151855"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "X"
      ],
      "id": "84338235-2a97-47bb-a3fb-abfb857d4aba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   이론적인 결과와 일치\n",
        "\n",
        "## 잠깐 생각해보자..\n",
        "\n",
        "`-` 결국 위의 예제에 한정하여 임의의 ${\\bf \\hat{W}}$에 대한\n",
        "$\\frac{\\partial}{\\partial {\\bf \\hat W}}loss$는 아래와 같이 계산할 수\n",
        "있다.\n",
        "\n",
        "-   (단계1) $2{\\bf v}$를 계산하고\n",
        "-   (단계2) (단계1)의 결과 앞에 $-{\\bf I}$를 곱하고\n",
        "-   (단계3) (단계2)의 결과 앞에 ${\\bf X}^\\top$를 곱한다.\n",
        "\n",
        "`-` step1에서 ${\\bf v}$는 어떻게 알지?\n",
        "\n",
        "-   X $\\to$ u=X@W $\\to$ v = y-u\n",
        "\n",
        "-   그런데 이것은 우리가 loss를 구하기 위해서 이미 계산해야 하는것\n",
        "    아니었나?\n",
        "\n",
        "-   step1: yhat, step2: loss, step3: derivate, step4: update\n",
        "\n",
        "`-` **(중요)** step2에서 loss만 구해서 저장할 생각 하지말고 중간과정을\n",
        "다 저장해라. (그중에 v와 같이 필요한것이 있을테니까) 그리고 그걸 적당한\n",
        "방법을 통하여 이용하여 보자.\n",
        "\n",
        "### backprogation 알고리즘 모티브\n",
        "\n",
        "`-` 아래와 같이 함수의 변환을 아키텍처로 이해하자.\n",
        "(함수의입력=레이어의입력, 함수의출력=레이어의출력)\n",
        "\n",
        "-   ${\\bf X} \\overset{l1}{\\to} {\\bf X}{\\bf W} \\overset{l2}{\\to} {\\bf y} -{\\bf X}{\\bf W} \\overset{l3}{\\to} ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$\n",
        "\n",
        "`-` 그런데 위의 계산과정을 아래와 같이 요약할 수도 있다.\n",
        "(${\\bf X} \\to {\\bf \\hat y} \\to loss$가 아니라\n",
        "${\\bf W} \\to loss({\\bf W})$로 생각해보세요)\n",
        "\n",
        "-   ${\\bf W} \\overset{l1}{\\to} {\\bf u} \\overset{l2}{\\to} {\\bf v} \\overset{l3}{\\to} loss$\n",
        "\n",
        "`-` 그렇다면 아래와 같은 사실을 관찰할 수 있다.\n",
        "\n",
        "-   (단계1) $2{\\bf v}$는 function of ${\\bf v}$이고, ${\\bf v}$는 l3의\n",
        "    입력 (혹은 l2의 출력)\n",
        "-   (단계2) $-{\\bf I}$는 function of ${\\bf u}$이고, ${\\bf u}$는 l2의\n",
        "    입력 (혹은 l1의 출력)\n",
        "-   (단계3) 마찬가지의 논리로 ${\\bf X}^\\top$는 function of ${\\bf W}$로\n",
        "    해석할 수 있다.\n",
        "\n",
        "`-` 요약: $2{\\bf v},-{\\bf I}, {\\bf X}^\\top$와 같은 핵심적인 값들이 사실\n",
        "각 층의 입/출력 값들의 함수꼴로 표현가능하다. $\\to$ 각 층의 입/출력\n",
        "값들을 모두 기록하면 미분계산을 유리하게 할 수 있다.\n",
        "\n",
        "-   문득의문: 각 층의 입출력값 ${\\bf v}, {\\bf u}, {\\bf W}$로 부터\n",
        "    $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$ 를 만들어내는 방법을 모른다면\n",
        "    헛수고 아닌가?\n",
        "-   의문해결: 어차피 우리가 쓰는 층은 선형+(렐루, 시그모이드, …) 정도가\n",
        "    전부임. 따라서 변환규칙은 미리 계산할 수 있음.\n",
        "\n",
        "`-` 결국\n",
        "\n",
        "`(1)` 순전파를 하면서 입출력값을 모두 저장하고\n",
        "\n",
        "`(2)` 그에 대응하는 층별 미분계수값 $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$\n",
        "를 구하고\n",
        "\n",
        "`(3)` 층별미분계수값을 다시 곱하면 (그러니까\n",
        "${\\bf X}^\\top (-{\\bf I}) 2{\\bf v}$ 를 계산) 된다.\n",
        "\n",
        "### backpropagation\n",
        "\n",
        "`(1)` 순전파를 계산하고 각 층별 입출력 값을 기록\n",
        "\n",
        "-   yhat = net(X)\n",
        "-   loss = loss_fn(yhat,y)\n",
        "\n",
        "`(2)` 역전파를 수행하여 손실함수의 미분값을 계산\n",
        "\n",
        "-   loss.backward()\n",
        "\n",
        "`-` 참고로 (1)에서 층별 입출력값은 GPU의 메모리에 기록된다.. 무려 GPU\n",
        "메모리..\n",
        "\n",
        "`-` 작동원리를 GPU의 관점에서 요약 (슬기로운 GPU 활용)\n",
        "\n",
        "***gpu특징: 큰 차원의 매트릭스 곱셈 전문가 (원리? 어마어마한\n",
        "코어숫자)***\n",
        "\n",
        "-   아키텍처 설정: 모형의 파라메터값을 GPU 메모리에 올림 //\n",
        "    `net.to(\"cuda:0\")`\n",
        "-   순전파 계산: ***중간 계산결과를 모두 GPU메모리에 저장*** (순전파\n",
        "    계산을 위해서라면 굳이 GPU에 있을 필요는 없으나 후에 역전파를\n",
        "    계산하기 위한 대비) // `net(X)`\n",
        "-   오차 및 손실함수 계산: `loss = loss_fn(yhat,y)`\n",
        "-   역전파 계산: ***순전파단계에서 저장된 계산결과를 활용***하여\n",
        "    손실함수의 미분값을 계산 // `loss.backward()`\n",
        "-   다음 순전파 계산: ***이전값은 삭제하고 새로운 중간계산결과를\n",
        "    GPU메모리에 올림***\n",
        "-   반복.\n",
        "\n",
        "## some comments\n",
        "\n",
        "`-` 역전파기법은 체인룰 + $\\alpha$ 이다.\n",
        "\n",
        "`-` 오차역전파기법이라는 용어를 쓰는 사람도 있다.\n",
        "\n",
        "`-` 이미 훈련한 네트워크에 입력 $X$를 넣어 결과값만 확인하고 싶을 경우\n",
        "순전파만 사용하면 되고, 이 상황에서는 좋은 GPU가 필요 없다.\n",
        "\n",
        "# 기울기소멸\n",
        "\n",
        "## 고요속의 외침\n",
        "\n",
        "`-` <https://www.youtube.com/watch?v=ouitOnaDtFY>\n",
        "\n",
        "`-` 중간에 한명이라도 잘못 말한다면..\n",
        "\n",
        "## 정의\n",
        "\n",
        "`-` In machine learning, the vanishing gradient problem is encountered\n",
        "when training artificial neural networks with gradient-based learning\n",
        "methods and backpropagation.\n",
        "\n",
        "## 이해\n",
        "\n",
        "`-` 당연한것 아닌가?\n",
        "\n",
        "-   그레디언트 기반의 학습 (그레디언트 기반의 옵티마이저): 손실함수의\n",
        "    기울기를 통하여 업데이트 하는 방식\n",
        "-   역전파: 손실함수의 기울기를 구하는 테크닉 (체인룰 + $\\alpha$).\n",
        "    구체적으로는 (1) 손실함수를 여러단계로 쪼개고 (2) 각 단계의 미분값을\n",
        "    각각 구하고 (3) 그것들을 모두 곱하여 기울기를 계산한다.\n",
        "-   0 근처의 숫자를 계속 곱하면 터지거나 0으로 간다. (사실 안정적인\n",
        "    기울기가 나올 것이라고 생각하는것 자체가 이상함)"
      ],
      "id": "dffeef69-565d-440f-946a-08126601da17"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np "
      ],
      "id": "52fbd89c-dc57-41c2-be75-31ff2d98e3c3"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "grads = np.random.uniform(low=-2,high=2,size=100) \n",
        "grads"
      ],
      "id": "95ce5cfc-eed8-4acf-b927-12a929125508"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "grads.prod()"
      ],
      "id": "92b87c0d-6c57-4f92-afed-27b622bf5c61"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   기울기가 소멸함"
      ],
      "id": "aebf8e56-a143-4b9c-beca-45f3f0d5369d"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "grads = np.random.uniform(low=-5,high=5,size=100) \n",
        "grads.prod()"
      ],
      "id": "b17f42bf-9be7-4b3c-a466-64bf8407bd42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   기울기가 폭발함."
      ],
      "id": "309e54a1-6d2f-4268-adb7-6ee424e0604f"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "grads = np.random.uniform(low=-1,high=3.5,size=100) \n",
        "grads.prod()"
      ],
      "id": "11615aac-a8c2-435d-8c91-ad6b297155de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 도깨비: 기울기가 소멸하기도 하고 터지기도 한다.\n",
        "\n",
        "## [해결책](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) (기울기 소멸에 대한 해결책)\n",
        "\n",
        "`-` Multi-level hierarchy\n",
        "\n",
        "-   여러층을 쪼개서 학습하자 $\\to$ 어떻게? 사전학습, 층벼학습\n",
        "-   기울기소실문제를 해결하여 딥러닝을 유행시킨 태초의(?) 방법임.\n",
        "-   결국 입력자료를 바꾼뒤에 학습하는 형태\n",
        "\n",
        "`-` Gradient clipping\n",
        "\n",
        "-   너무 큰 값의 기울기는 사용하지 말자. (기울기 폭발에 대한 대비책)\n",
        "\n",
        "`-` Faster hardware\n",
        "\n",
        "-   GPU를 중심으로 한 테크닉\n",
        "-   근본적인 문제해결책은 아니라는 힌튼의 비판\n",
        "-   CPU를 쓸때보다 GPU를 쓰면 약간 더 깊은 모형을 학습할 수 있다 정도?\n",
        "\n",
        "`-` Residual Networks, LSTM\n",
        "\n",
        "-   아키텍처를 변경하는 방법\n",
        "\n",
        "`-` Other activation functions\n",
        "\n",
        "-   렐루의 개발\n",
        "\n",
        "`-` 배치정규화\n",
        "\n",
        "-   어쩌다보니 되는것.\n",
        "-   배치정규화는 원래 공변량 쉬프트를 잡기 위한 방법임. 그런데 기울기\n",
        "    소멸에도 효과가 있음. 현재는 기울기소멸문제에 대한 해결책으로\n",
        "    빠짐없이 언급되고 있음. 2015년의 원래 논문에는 기울기소멸에 대한\n",
        "    언급은 없었음. (https://arxiv.org/pdf/1502.03167.pdf)\n",
        "-   심지어 배치정규화는 오버피팅을 잡는효과도 있음 (이것은 논문에\n",
        "    언급했음)\n",
        "\n",
        "`-` **기울기를 안구하면 안되나?**\n",
        "\n",
        "-   베이지안 최적화기법: (https://arxiv.org/pdf/1807.02811.pdf) $\\to$\n",
        "    GPU를 어떻게 쓰지? $\\to$ 느리다"
      ],
      "id": "30a18b8a-afad-4b76-871d-39dbc14bda49"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    }
  }
}